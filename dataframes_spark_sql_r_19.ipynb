{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@68d24914\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://10.142.0.5:4049)\" target=\"new_tab\">Spark UI: local-1665504227098</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1665504227098: Some(http://10.142.0.5:4049)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"SampleKMeans\") \n",
    "             .master(\"local[*]\")\n",
    "             .getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.ml.clustering._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 32|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "caseClassDS = [name: string, age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "import spark.implicits._\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /data/spark/people.json\n",
       "peopleDS = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/data/spark/people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@68d24914\n",
       "defined class Person\n",
       "txtRDD = /data/spark/people.txt MapPartitionsRDD[7] at textFile at <console>:24\n",
       "arrayRDD = MapPartitionsRDD[8] at map at <console>:25\n",
       "personRDD = MapPartitionsRDD[9] at map at <console>:26\n",
       "peopleDF = [name: string, age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark\n",
    "import spark2.implicits._\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val txtRDD = sc.textFile(\"/data/spark/people.txt\")\n",
    "val arrayRDD = txtRDD.map(_.split(\",\"))\n",
    "val personRDD = arrayRDD.map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n",
    "val peopleDF = personRDD.toDF()\n",
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schemaString = name age\n",
       "fieldsArray = Array(name, age)\n",
       "fields = Array(StructField(name,StringType,true), StructField(age,StringType,true))\n",
       "schema = StructType(StructField(name,StringType,true), StructField(age,StringType,true))\n",
       "peopleRDD = /data/spark/people.txt MapPartitionsRDD[14] at textFile at <console>:65\n",
       "rowRDD = MapPartitionsRDD[16] at map at <console>:66\n",
       "peopleDF = [name: string, age: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "val fieldsArray = schemaString.split(\" \")\n",
    "val fields = fieldsArray.map(\n",
    "    name => StructField(name, StringType, nullable = true)\n",
    "    )\n",
    "val schema = StructType(fields)\n",
    "val peopleRDD = spark.sparkContext.textFile(\"/data/spark/people.txt\")\n",
    "val rowRDD = peopleRDD.map(_.split(\",\")).map(\n",
    "    attributes => Row(attributes(0), attributes(1).trim)\n",
    "    )\n",
    "val peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %AddDeps my.company artifact-id version\n",
      "\n",
      "Option                        Description                          \n",
      "------                        -----------                          \n",
      "--abort-on-resolution-errors  Abort (no downloads) when resolution \n",
      "                                fails                              \n",
      "--classifier                  Sets the dependency's classifier     \n",
      "--credential                  Adds a credential file to be used to \n",
      "                                the list                           \n",
      "--exclude                     exclude dependency                   \n",
      "--ivy-configuration           Sets the Ivy configuration for the   \n",
      "                                dependency; defaults to \"default\"  \n",
      "--repository                  Adds an additional repository to     \n",
      "                                available list                     \n",
      "--trace                       Prints out trace of download progress\n",
      "--transitive                  Retrieve dependencies recursively    \n",
      "--verbose                     Prints out additional information    \n"
     ]
    }
   ],
   "source": [
    "%AddDeps com.databricks:spark-avro_2.11:4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:590)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:325)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import spark2.implicits._\n",
    "val df = spark.read.format(\"com.databricks.spark.avro\").load(\"/data/spark/episodes.avro\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking com.databricks:spark-xml_2.11:0.12.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir7592072976879876760/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir7592072976879876760/toree_add_deps/https/repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.12.0/spark-xml_2.11-0.12.0.jar\n",
      "-> New file at /tmp/toree-tmp-dir7592072976879876760/toree_add_deps/https/repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.12.0/spark-xml_2.11-0.12.0.pom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%AddDeps com.databricks spark-xml_2.11 0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|  _id|              author|         description|          genre|price|publish_date|               title|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|bk101|Gambardella, Matthew|\n",
      "\n",
      "\n",
      "         An in...|       Computer|44.95|  2000-10-01|XML Developer's G...|\n",
      "|bk102|          Ralls, Kim|A former architec...|        Fantasy| 5.95|  2000-12-16|       Midnight Rain|\n",
      "|bk103|         Corets, Eva|After the collaps...|        Fantasy| 5.95|  2000-11-17|     Maeve Ascendant|\n",
      "|bk104|         Corets, Eva|In post-apocalyps...|        Fantasy| 5.95|  2001-03-10|     Oberon's Legacy|\n",
      "|bk105|         Corets, Eva|The two daughters...|        Fantasy| 5.95|  2001-09-10|  The Sundered Grail|\n",
      "|bk106|    Randall, Cynthia|When Carla meets ...|        Romance| 4.95|  2000-09-02|         Lover Birds|\n",
      "|bk107|      Thurman, Paula|A deep sea diver ...|        Romance| 4.95|  2000-11-02|       Splish Splash|\n",
      "|bk108|       Knorr, Stefan|An anthology of h...|         Horror| 4.95|  2000-12-06|     Creepy Crawlies|\n",
      "|bk109|        Kress, Peter|After an inadvert...|Science Fiction| 6.95|  2000-11-02|        Paradox Lost|\n",
      "|bk110|        O'Brien, Tim|Microsoft's .NET ...|       Computer|36.95|  2000-12-09|Microsoft .NET: T...|\n",
      "|bk111|        O'Brien, Tim|The Microsoft MSX...|       Computer|36.95|  2000-12-01|MSXML3: A Compreh...|\n",
      "|bk112|         Galos, Mike|Microsoft Visual ...|       Computer|49.95|  2001-04-16|Visual Studio 7: ...|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"xml\").option(\"rowTag\",\"book\").load(\"/data/spark/books.xml\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
